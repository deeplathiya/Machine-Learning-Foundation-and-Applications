{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download FMNIST dataset from torchvision.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainSet = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "testSet = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3 separate models having the following configuration:\n",
    "#a. 784-256-10\n",
    "#b. 784-203-203-10\n",
    "#c. 784-176-176-176-10\n",
    "\n",
    "#Define the model class\n",
    "class Model_a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_a, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Model_b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_b, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 203)\n",
    "        self.fc2 = nn.Linear(203, 203)\n",
    "        self.fc3 = nn.Linear(203, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Model_c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_c, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 176)\n",
    "        self.fc2 = nn.Linear(176, 176)\n",
    "        self.fc3 = nn.Linear(176, 176)\n",
    "        self.fc4 = nn.Linear(176, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the training function\n",
    "\n",
    "def train(model, trainSet, lr, epochs, batch_size):\n",
    "      training_loss = list()\n",
    "      trainset = t.utils.data.DataLoader(trainSet, batch_size=batch_size, shuffle=True)\n",
    "      testset = t.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=True)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "      model.train()\n",
    "      for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            for images, labels in trainset:\n",
    "                  optimizer.zero_grad()\n",
    "                  output = model(images)\n",
    "                  loss = criterion(output, labels)\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                  running_loss += loss.item()\n",
    "            e_loss = running_loss/len(trainset)\n",
    "            training_loss.append(1 if e_loss > 1 else e_loss)\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\"Training Loss: {:.3f}.. \".format(running_loss/len(trainset)))\n",
    "      return training_loss\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train each model with the following learning rates (use SGD with momentum=0.9)\n",
    "# with batch size set to 64:\n",
    "# a. 0.0001\n",
    "# b. 0.001\n",
    "# c. 0.01\n",
    "# d. 0.1\n",
    "# e. 1\n",
    "\n",
    "lr = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "print(\"-----------------------------------------------Model_a----------------------------------------------------\")\n",
    "lossa_3 = list()\n",
    "model_a = Model_a()\n",
    "for i in lr:\n",
    "      print(\"Learning Rate: \", i)\n",
    "      lossa_3.append(train(model_a, trainSet, i, epochs, batch_size))\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------Model_b----------------------------------------------------\")\n",
    "lossb_3 = list()\n",
    "model_b = Model_b()\n",
    "for i in lr:\n",
    "      print(\"Learning Rate: \", i)\n",
    "      lossb_3.append(train(model_b, trainSet, i, epochs, batch_size))\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------Model_c----------------------------------------------------\")\n",
    "model_c = Model_c()\n",
    "lossc_3 = list()\n",
    "for i in lr:\n",
    "      print(\"Learning Rate: \", i)\n",
    "      lossc_3.append(train(model_c, trainSet, i, epochs, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fix the learning rate at 0.01 and try the following different batch sizes:\n",
    "# a. 16\n",
    "# b. 64\n",
    "# c. 256\n",
    "# d. 1024\n",
    "# e. 2048\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = [16, 64, 256, 1024, 2048]\n",
    "\n",
    "print(\"-----------------------------------------------Model_a----------------------------------------------------\")\n",
    "lossa_4 = list()\n",
    "model_a = Model_a()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    lossa_4.append(train(model_a, trainSet, lr, epochs, i))\n",
    "\n",
    "print(\"-----------------------------------------------Model_b----------------------------------------------------\")\n",
    "lossb_4 = list()\n",
    "model_b = Model_b()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    lossb_4.append(train(model_b, trainSet, lr, epochs, i))\n",
    "\n",
    "print(\"-----------------------------------------------Model_c----------------------------------------------------\")\n",
    "lossc_4 = list()\n",
    "model_c = Model_c()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    lossc_4.append(train(model_c, trainSet, lr, epochs, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each experiment in point 3, plot the training loss vs epochs graph. Only one graph \n",
    "# should be generated per model containing 5 different plots with corresponding \n",
    "# learning rates clearly labelled\n",
    "\n",
    "# model_a\n",
    "plt.plot(lossa_3[0], label='0.0001')\n",
    "plt.plot(lossa_3[1], label='0.001')\n",
    "plt.plot(lossa_3[2], label='0.01')\n",
    "plt.plot(lossa_3[3], label='0.1')\n",
    "plt.plot(lossa_3[4], label='1')\n",
    "#increase the size of the plot\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Model_a')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# model_b\n",
    "plt.plot(lossb_3[0], label='0.0001')\n",
    "plt.plot(lossb_3[1], label='0.001')\n",
    "plt.plot(lossb_3[2], label='0.01')\n",
    "plt.plot(lossb_3[3], label='0.1')\n",
    "plt.plot(lossb_3[4], label='1')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Model_b')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# model_c\n",
    "plt.plot(lossc_3[0], label='0.0001')\n",
    "plt.plot(lossc_3[1], label='0.001')\n",
    "plt.plot(lossc_3[2], label='0.01')\n",
    "plt.plot(lossc_3[3], label='0.1')\n",
    "plt.plot(lossc_3[4], label='1')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Model_c')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For, point 4 report the validation accuracy for each model in a tabular form.\n",
    "\n",
    "def test_model(model, testSet, batch_size):\n",
    "    testset = t.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=True)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with t.no_grad():\n",
    "        for images, labels in testset:\n",
    "            outputs = model(images)\n",
    "            _, predicted = t.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "\n",
    "# model_a\n",
    "\n",
    "print(\"-----------------------------------------------Model_a----------------------------------------------------\")\n",
    "acc_a = list()\n",
    "model_a = Model_a()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    acc = test_model(model_a, testSet, i)\n",
    "    acc_a.append(acc)\n",
    "    print(\"Validation Accuracy: \", acc)\n",
    "\n",
    "\n",
    "# model_b\n",
    "\n",
    "print(\"-----------------------------------------------Model_b----------------------------------------------------\")\n",
    "acc_b = list()\n",
    "model_b = Model_b()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    acc = test_model(model_b, testSet, i)\n",
    "    acc_b.append(acc)\n",
    "    print(\"Validation Accuracy: \", acc)\n",
    "\n",
    "# model_c\n",
    "\n",
    "print(\"-----------------------------------------------Model_c----------------------------------------------------\")\n",
    "acc_c = list()\n",
    "model_c = Model_c()\n",
    "for i in batch_size:\n",
    "    print(\"Batch Size: \", i)\n",
    "    acc = test_model(model_c, testSet, i)\n",
    "    acc_c.append(acc)\n",
    "    print(\"Validation Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, increase the swap the model in 2(a) with the model 784-512-10. Use learning \n",
    "# rate 0.01 with batch size of 64 and train this model. Report the validation accuracy of \n",
    "# current model as well as the validation accuracy of the model in 2(a) trained using the \n",
    "# same learning rate and batch size\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "# model_d\n",
    "\n",
    "class Model_d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_d, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"-----------------------------------------------Model_d----------------------------------------------------\")\n",
    "acc_d = list()\n",
    "model_d = Model_d()\n",
    "lossa_5 = list()\n",
    "lossa_5.append(train(model_d, trainSet, lr, epochs, batch_size))\n",
    "acc = test_model(model_d, testSet, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now answer the following questions based on your experiments:\n",
    "\n",
    "# How does increasing the learning rate affect the training loss? Why?\n",
    "\n",
    "ans1 = \"The training loss decreases with increase in learning rate. This is because the learning rate is the step size of the gradient descent algorithm. If the learning rate is too small, the model will take a lot of time to converge to the minimum of the loss function. If the learning rate is too large, the model will not be able to converge to the minimum of the loss function. Hence, the learning rate should be chosen such that the model converges to the minimum of the loss function in the least number of epochs.\"\n",
    "\n",
    "# How does increasing the batch size affect the validation accuracy? Why?\n",
    "\n",
    "ans2 = \"The validation accuracy increases with increase in batch size. This is because the batch size is the number of samples that are used to update the weights of the model. If the batch size is too small, the model will take a lot of time to converge to the minimum of the loss function. If the batch size is too large, the model will not be able to converge to the minimum of the loss function. Hence, the batch size should be chosen such that the model converges to the minimum of the loss function in the least number of epochs.\"\n",
    "\n",
    "# How does increasing depth affect validation accuracy? Why?\n",
    "\n",
    "ans3 = \"The validation accuracy increases with increase in depth. This is because the depth of the model is the number of layers in the model. If the depth is too small, the model will not be able to learn the complex patterns in the data. If the depth is too large, the model will overfit the data. Hence, the depth should be chosen such that the model is able to learn the complex patterns in the data without overfitting the data.\"\n",
    "\n",
    "# How does increasing the number of parameters affect validation accuracy? Why?\n",
    "\n",
    "ans4 = \"The validation accuracy increases with increase in the number of parameters. This is because the number of parameters is the number of weights in the model. If the number of parameters is too small, the model will not be able to learn the complex patterns in the data. If the number of parameters is too large, the model will overfit the data. Hence, the number of parameters should be chosen such that the model is able to learn the complex patterns in the data without overfitting the data.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
